---
title: Volta's whitepaper メモ
tags: GPU
---

* <https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf>

## Tesla V100: The AI Computing and HPC Powerhouse

|            |             |
|------------|-------------|
|ダイサイズ  |815 mm^2     |
|トランジスタ|211億個      |
|プロセス    |TSMC 12nm FFN|

* Key features
    * New SM Architecture
        * テンソルコア
        * 整数、浮動小数点のdata pathが独立
            * アドレス計算しながらメモリアクセスなど
        * independent thread scheduling
        * L1キャッシュとshared memoryがくっついた
    * Second-Generation NVLink
    * HBM2 Memory
        * 16GB HMB2 memory: 900 GB/sec peak memory bandwidth
    * Volta Mutli-Process Service
    * Enhanced Unified Memory and Address Transition Services
    * Maximum Performance and Maximum Efficiency Modes
    * Cooperative Groups and New Cooperative Launch APIs
    * Volta Optimized Software

## NVIDIA GPUs: - The Fastest and Most Flexible Deep Learning Platform

## GV100 GPU Hardware Architecture In-Depth

GV100は複数のGPU Processing Clusters (**GPCs**)、Texture Processing Clusters (**TPCs**)、Streaming Multiprocessors (**SMs**)、memory controllerで構成されている。

GV100の構成
* 6個のGPCs
    それぞれのGPCsは次を含む
    * 7個のTPCs(それぞれのTPCsは2個のSMsを含む)
    * 計14個のSMs
* 計84個のSMs
    それぞれのSMsは次を含む
    * 64 FP32 cores
    * 64 INT32 cores
    * 32 FP64 cores
    * 8 Tensor cores
    * 4 texture units
* 8個の512-bit memory controllers
* L2 cache: 6144KB

|          |SMs|Memory Interface|Memory Size|L2 Cache Size|Shared Memory Size/SM   |Register File Size/SM|
|----------|---|----------------|-----------|-------------|------------------------|---------------------|
|Tesla V100|80 |4096-bit HMB2   |16 GB      |6144 KB      |Configurable up to 96 KB|256 KB               |


* Enhanced L1 Data Cache and Shared Memory
    * L1 cacheとshared memoryあわせて128 KB/SMある
    * L1 cacheはtextureとload/store操作が使用する
* Simultaneous Exectuion of FP32 and INT32 Operations
    * FP32のコアとINT32のコアを分離し、同時に実行することを可能とした
    * FMAも速くなり、4クロックしかかからない
    * ポインタ操作+浮動小数点演算のようなプログラムは恩恵を受ける

## GV100 CUDA Hardware and Software Architectural Advances

### Independent Thread Scheduling

* PAST(**SIMT**): single program counter shared amongst all 32 threads, combined with an **active mask**
    * この結果warp内でデータのやりとりするコードが非常に書きづらい(簡単にdeadlockに陥る)

Voltaからは**Independent Thread Scheduling**が導入され、
スレッドごとにプログラムカウンタ、コールスタックを持つようになった。
これによりwarp内の他のスレッドのデータを待つようなコードを書いても良くなった。
また、並列化効率を最大化するため、schedule optimizerがactive threadsのまとめ方を最適化する。

* Starvation-Free Algorithms
    * doubly linked list with fine-grained locks

### Volta Multi-Process Service

**PASS**

### Unified Memory and Address Translation Services

> A new Access Counter feature keeps track of the frequency of access that a GPU makes to memory located on other processor.

Access Counterによって、データはは最もアクセスの多いデバイスの物理的なメモリに配置されやすくなる
Voltaは**Adress Treanslation Service**(ATS) over NVLinkを提供しており、CPUのページテーブルを直接触れる。

### Cooperative Groups

* producer-consumer parallelism
* opportunistic parallelism
* global synchronization across the entire grid

coorperative groupを使えば、(PascalとVoltaでは)grid-wideやmutli-GPUでsyncできる。   
Voltaではさらにとても柔軟にsyncできて、例えば、cross-warp, sub-warpの粒度でsyncできる。

```cpp
__global__ void cooperative_kernel(...)
{
	thread_group my_block = this_thread_block();
	thread_group my_tile = tiled_partition(my_block, 32);
	// この操作はブロックの先頭の32スレッドでしか実行されない
	if (my_block.thread_rank() < 32) {
		/** ... **/
		my_tile.sync();
	}
}
```

`cuda-memcheck`といった競合検知ツールやCUDA debuggerを使えばバグを発見しやすい。
coorperative groupを使えばsyncパターンの幅が広がり、プログラミングしやすくなる。

(例: particle simulation)

1. 位置と速度を更新する
2. regular grid spatial data structureを構築し、衝突した粒子を発見する

cooperative gropuがない時は、複数回カーネルを起動するしかなかった。
なぜならスレッドがphase1からphase2へ移行するのを自然に表現するにはそれしかなかったので。

```cpp
integrate<<<blocks, threads, 0, s>>>(particles); // phase1
collide<<<blocks, threads, 0, s>>>(particles);   // phase2
```
coorperative groupがあれば次のように書ける

```cpp
__global__ void particleSim(Particle* p, int N) {
	grid_group g = this_grid();
	// phase1
	for (i = g.thread_rank() ; i < N ; i += g.size())
		integrate(p[i]);
	// phase2
	for (i = g.thread_rank() ; i < N ; i += g.size())
		collide(p[i], p, N);
```

mutli-GPUも同様に可能である。

* [`cudaLaunchCooperativeKernel()`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1g504b94170f83285c71031be6d5d15f73)
* [`cudaLaunchCooperativeKernelMultiDevice()`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1g20f8d75d8786c54cc168c47fde66ee52)


